# Prueba: entreno 60% de los negativos con los parámetros que devolvió la BO 4.
rm(list=ls())

library(data.table)
library(ggplot2)
library(devtools)
library(Rcpp)
library(lightgbm)
library(methods)
library(ROCR)
library(knitr)
library(mlrMBO)
library(readr)
library(DiceKriging)

rm(list=ls())

# Carga del ds 
setwd("/home/antonellaalopez/buckets/b1/datasetsOri/")

kcampos_separador               <-  "\t"
karchivo_entrada_full     <-  "paquete_premium.txt.gz"
ds <- fread(karchivo_entrada_full, header=TRUE, sep=kcampos_separador)


ds[ , clase01 :=  ifelse( clase_ternaria=="BAJA+2", 1L, 0L)  ]
ds[ , clase_ternaria :=  NULL  ]
ds = ds[foto_mes>201712 , ]


# Ensayo 1: Corrección de columnas que a priori son malas
source_url("https://raw.githubusercontent.com/fedefliguer/maestria_DMEyF_TP02/main/scripts/correcciones.R")
correcciones(ds)

# Ensayo 2: Nuevas columnas armadas por Fede
source_url("https://raw.githubusercontent.com/fedefliguer/maestria_DMEyF_TP02/main/scripts/nuevas_columnas_fede.R")
nuevas_columnas_fede(ds)

# Ensayo 3: Nuevas columnas armadas por cátedra
source_url("https://raw.githubusercontent.com/fedefliguer/maestria_DMEyF_TP02/main/scripts/nuevas_columnas_catedra.R")
nuevas_columnas_catedra(ds)

base = ds

# Si quiero quedarme únicamente con las n variables más importantes según los modelos 1 a 5
source_url("https://raw.githubusercontent.com/fedefliguer/maestria_DMEyF_TP02/main/scripts/solo_importantes.R")
nu_variables = 150
solo_importantes(ds, nu_variables)

# Agrego el lag 1 y el delta 1 para todas las variables seleccionadas
source_url("https://raw.githubusercontent.com/fedefliguer/maestria_DMEyF_TP02/main/scripts/lag_delta.R")
lag_delta(ds)

# Agrego históricas (maximo, minimo, tendencia) para las ventanas que quiera
ventanas = c(12)
source_url("https://raw.githubusercontent.com/fedefliguer/maestria_DMEyF_TP02/main/scripts/variables_historicas.R")
variables_historicas(ds, ventanas)

ds_150 = ds

# SOLO PARA LAS 50 MEJORES
ds = base

source_url("https://raw.githubusercontent.com/fedefliguer/maestria_DMEyF_TP02/main/scripts/solo_importantes.R")
nu_variables = 50
solo_importantes(ds, nu_variables)

ventanas = c(3, 6)
source_url("https://raw.githubusercontent.com/fedefliguer/maestria_DMEyF_TP02/main/scripts/variables_historicas.R")
variables_historicas(ds, ventanas)

ds_50 = ds[ , grepl( "__" , names( ds ) ), with = FALSE ]

dataset = cbind(ds_150, ds_50)

rm(ds_150, ds_50)

#los campos que se van a utilizar, intencionalmente no uso  numero_de_cliente
campos_buenos  <- setdiff(  colnames(dataset) ,  c("clase_ternaria","clase01","numero_de_cliente") )

############# UNDERSAMPLING Y LGB FORMAT

#hago undersampling de los negativos
#me quedo con TODOS los positivos, pero con solo el 5% de los negativos
set.seed(102191)
dataset[ , azar:= runif( nrow(dataset) ) ]

dataset[ ( foto_mes>=201701 & foto_mes<=202003 & foto_mes!=201905 & ( clase01==1 | azar<=0.50) ),  BO_train := 1L]

#Testeo en 201902, el mismo mes pero un año antes
dataset[ foto_mes==201905,  BO_test1 := 1L]

rm(base, ds)

start.time <- Sys.time()


#dejo los datos en el formato que necesita LightGBM
dBO_train  <- lgb.Dataset( data  = data.matrix(  dataset[ BO_train==1, campos_buenos, with=FALSE]),
                           label = dataset[ BO_train==1, clase01],
                           free_raw_data = TRUE
)

dBO_test1  <- lgb.Dataset( data  = data.matrix(  dataset[ BO_test1==1, campos_buenos, with=FALSE]),
                           label = dataset[ BO_test1==1, clase01],
                           free_raw_data = TRUE
)

dataset_aplicacion <- copy( dataset[ foto_mes==202005, ] )
#dataset_aplicacion_enero <- copy( dataset[ foto_mes==202001, ] )
#dataset_aplicacion_febrero <- copy( dataset[ foto_mes==202002, ] )

fganancia_logistic_lightgbm   <- function(probs, data) 
{
  vlabels <- getinfo(data, "label")
  
  tbl <- as.data.table( list( "prob"=probs, "gan"= ifelse( vlabels==1, 29250, -750 ) ) )
  
  setorder( tbl, -prob )
  tbl[ , gan_acum :=  cumsum( gan ) ]
  gan <- max( tbl$gan_acum )
  
  return(  list( name= "ganancia", value=  gan, higher_better= TRUE ) )
}

estimar_lightgbm <- function( x )
{
  modelo <-  lgb.train(data= dBO_train,
                       objective= "binary",  #la clase es binaria
                       eval= fganancia_logistic_lightgbm,  #esta es la fuciona optimizar
                       valids= list( valid1= dBO_test1 ),
                       first_metric_only= TRUE,
                       metric= "custom",  #ATENCION   tremendamente importante
                       num_iterations=  999999,  #un numero muy grande
                       early_stopping_rounds=  200,
                       min_data_in_leaf= as.integer( x$pmin_data_in_leaf ),
                       feature_fraction= x$pfeature_fraction,
                       learning_rate= x$plearning_rate,
                       feature_pre_filter= FALSE,
                       verbose= -1,
                       seed= 102191
  )
  
  ganancia1  <- unlist(modelo$record_evals$valid1$ganancia$eval)[ modelo$best_iter ] 
  
  #esta es la forma de devolver un parametro extra
  attr(ganancia1 ,"extras" ) <- list("pnum_iterations"= modelo$best_iter )
  
  cat( modelo$best_iter, ganancia1, "\n" )
  
  return( ganancia1 )
}


#setwd("/home/fjf_arg_gcloud/buckets/b1/tp2-working/work")
setwd("/home/antonellaalopez/buckets/b1/work/")


#calculo el modelo final
modelo_final <- lgb.train(data= dBO_train,
                          objective= "binary",
                          eval= fganancia_logistic_lightgbm,
                          valids= list( valid1= dBO_test1 ),
                          first_metric_only= TRUE,
                          metric= "custom",
                          num_iterations=  999999,
                          early_stopping_rounds=  200,
                          learning_rate= 0.01009835,  #ATENCION, este es el valor que se cambia
                          min_data_in_leaf= as.integer(1886.936),
                          feature_pre_filter= FALSE,
                          feature_fraction= 0.3725627,
                          lambda_l1= 7.213174,
                          lambda_l2= 99.79456,
                          verbose= -1,
                          seed= 102191
)

campos_lags_ok = setdiff(  colnames(dataset_aplicacion) ,  c("azar","BO_train", "BO_test1", "clase01", "numero_de_cliente") )

# LO ARMO PARA EL TARGET

entrega <- copy( dataset_aplicacion )
prediccion_202005  <- predict( modelo_final,  data.matrix( entrega[  , campos_lags_ok, with=FALSE]), type = "prob")
entrega <-   as.data.table(cbind( "numero_de_cliente" = dataset_aplicacion[ foto_mes==202005, numero_de_cliente],  "prob" = prediccion_202005) )
entrega = entrega[order(-prob)]

entrega[  ,  estimulo :=  c(rep(1, 4000),rep(0,nrow(entrega)-4000))]
fwrite( entrega[ ,  c("numero_de_cliente", "estimulo"), with=FALSE], sep=",",  file="lgb_bo6_conFE_historicas_4000.csv")

entrega[  ,  estimulo :=  c(rep(1, 5000),rep(0,nrow(entrega)-5000))]
fwrite( entrega[ ,  c("numero_de_cliente", "estimulo"), with=FALSE], sep=",",  file="lgb_bo6_conFE_historicas_5000.csv")

entrega[  ,  estimulo :=  c(rep(1, 5500),rep(0,nrow(entrega)-5500))]
fwrite( entrega[ ,  c("numero_de_cliente", "estimulo"), with=FALSE], sep=",",  file="lgb_bo6_conFE_historicas_5500.csv")

entrega[  ,  estimulo :=  c(rep(1, 6000),rep(0,nrow(entrega)-6000))]
fwrite( entrega[ ,  c("numero_de_cliente", "estimulo"), with=FALSE], sep=",",  file="lgb_bo6_conFE_historicas_6000.csv")

entrega[  ,  estimulo :=  c(rep(1, 6500),rep(0,nrow(entrega)-6500))]
fwrite( entrega[ ,  c("numero_de_cliente", "estimulo"), with=FALSE], sep=",",  file="lgb_bo6_conFE_historicas_6500.csv")

entrega[  ,  estimulo :=  c(rep(1, 7000),rep(0,nrow(entrega)-7000))]
fwrite( entrega[ ,  c("numero_de_cliente", "estimulo"), with=FALSE], sep=",",  file="lgb_bo6_conFE_historicas_7000.csv")

entrega[  ,  estimulo :=  c(rep(1, 8000),rep(0,nrow(entrega)-8000))]
fwrite( entrega[ ,  c("numero_de_cliente", "estimulo"), with=FALSE], sep=",",  file="lgb_bo6_conFE_historicas_8000.csv")

lgb.save(modelo_final, "modelo_borradorDL_BO6_conFE_conHistoria.txt")

end.time <- Sys.time()
tiempo_ejecucion = end.time - start.time
print(tiempo_ejecucion)



# 0. 4000 me da 
# 1. 5000 me da 
# 2. 5500 me da 12.36722
# 3. 6000 me da 13.20720
# 4. 6500 me da 12.99346
# 5. 7000 me da 12.74596
# 6. 8000 me da 12.77971
